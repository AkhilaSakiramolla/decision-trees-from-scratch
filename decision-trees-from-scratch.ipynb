{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5sYQ9ubOzxV"
   },
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C6_XMnRFhqrW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import math\n",
    "from pprint import pprint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# To execute a cell line by line\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GT_RQMe3Zqu8"
   },
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRQP8i_DZ0Px"
   },
   "source": [
    "Consider the problem of predicting if a person has a college degree based on age and salary. The table\n",
    "and graph below contain training data for 10 individuals. Now build a decision tree for classifying whether a person has a college degree by greedily choosing threshold splits that maximize information\n",
    "gain. What is the depth of your tree and the information gain at each split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "ATzOQn4IZ4Ak",
    "outputId": "a06da7ef-2e5d-4692-c282-8dc07b5d3b0f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary ($)</th>\n",
       "      <th>College Degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>40000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>52000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>25000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>77000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>48000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>52</td>\n",
       "      <td>110000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>22</td>\n",
       "      <td>38000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>43</td>\n",
       "      <td>44000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>52</td>\n",
       "      <td>27000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>48</td>\n",
       "      <td>65000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Salary ($)  College Degree\n",
       "0   24       40000               1\n",
       "1   53       52000               0\n",
       "2   23       25000               0\n",
       "3   25       77000               1\n",
       "4   32       48000               1\n",
       "5   52      110000               1\n",
       "6   22       38000               1\n",
       "7   43       44000               0\n",
       "8   52       27000               0\n",
       "9   48       65000               1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "\n",
    "Age = np.array([24, 53, 23, 25, 32, 52, 22, 43, 52, 48])\n",
    "Salary = np.array([40000, 52000, 25000, 77000, 48000, 110000, 38000, 44000, 27000, 65000])\n",
    "College_Degree = np.array([1, 0, 0, 1, 1, 1, 1, 0, 0, 1])\n",
    "df = pd.DataFrame(list(zip(Age, Salary, College_Degree)), columns = ['Age', 'Salary ($)', 'College Degree'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "KJFX7Plfa3kw"
   },
   "outputs": [],
   "source": [
    "cols = ['Age','Salary ($)']\n",
    "X = np.array(list(zip(Age,Salary)))\n",
    "y = College_Degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFBd2aJaamkO"
   },
   "source": [
    "### 1) Decision tree implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "htexh5lIapbI"
   },
   "outputs": [],
   "source": [
    "# Entropy calculation\n",
    "\n",
    "def entropy(cls, tot):\n",
    "  return -(cls*1.0/tot)*math.log(cls*1.0/tot, 2)\n",
    "\n",
    "def entropyCalc(c1, c2):\n",
    "  if c1== 0 or c2 == 0:  # when there is only one class in the group, entropy is 0\n",
    "    return 0\n",
    "  return entropy(c1, c1+c2) + entropy(c2, c1+c2)\n",
    "\n",
    "def entropyDiv(div): \n",
    "  s = 0\n",
    "  n = len(div)\n",
    "  classes = set(div)\n",
    "  for c in classes:   # for each class, get entropy\n",
    "    n_c = sum(div==c)\n",
    "    e = n_c*1.0/n * entropyCalc(sum(div==c), sum(div!=c)) # weighted avg\n",
    "    s += e\n",
    "  return s, n\n",
    "\n",
    "def get_entropy(y_predict, y_real):\n",
    "  if len(y_predict) != len(y_real):\n",
    "    print('They have to be the same length')\n",
    "    return None\n",
    "  n = len(y_real)\n",
    "  s_true, n_true = entropyDiv(y_real[y_predict]) # left hand side entropy\n",
    "  s_false, n_false = entropyDiv(y_real[~y_predict]) # right hand side entropy\n",
    "  entropy = n_true*1.0/n * s_true + n_false*1.0/n * s_false # overall entropy, again weighted average\n",
    "  return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "2agExueVarcT"
   },
   "outputs": [],
   "source": [
    "class DecisionTreeAlgo():\n",
    "\n",
    "  def __init__(self, max_depth):\n",
    "    self.depth = 0\n",
    "    self.max_depth = max_depth\n",
    "    \n",
    "  def fit(self, x, y, node={}, depth=0):\n",
    "    if node is None: \n",
    "      return None\n",
    "    elif len(y) == 0:\n",
    "      return None\n",
    "    elif self.all_same(y):\n",
    "      return {'val':y[0]}\n",
    "    elif depth >= self.max_depth:\n",
    "        return None\n",
    "    else: \n",
    "        col, cutoff, entropy = self.allBestSplit(x, y)    # find one split given an information gain\n",
    "        print(\"Information gain : \", round(info_gain,2))\n",
    "        y_left = y[x[:, col] < cutoff]\n",
    "        y_right = y[x[:, col] >= cutoff]\n",
    "        node = {'col': cols[col], 'index_col':col,'cutoff':cutoff,'val': np.round(np.mean(y))}\n",
    "        node['left'] = self.fit(x[x[:, col] < cutoff], y_left, {}, depth+1)\n",
    "        node['right'] = self.fit(x[x[:, col] >= cutoff], y_right, {}, depth+1)\n",
    "        self.depth += 1 \n",
    "        self.trees = node\n",
    "        return node\n",
    "    \n",
    "  def allBestSplit(self, x, y):\n",
    "    col = None\n",
    "    min_entropy = 1\n",
    "    cutoff = None\n",
    "    for i, c in enumerate(x.T):\n",
    "        entropy, cur_cutoff = self.bestSplit(c, y)\n",
    "        if entropy == 0:    # find the first perfect cutoff. Stop Iterating\n",
    "            return i, cur_cutoff, entropy\n",
    "        elif entropy <= min_entropy:\n",
    "            min_entropy = entropy\n",
    "            col = i\n",
    "            cutoff = cur_cutoff\n",
    "    return col, cutoff, min_entropy\n",
    "\n",
    "  def bestSplit(self, col, y):\n",
    "    min_entropy = 10\n",
    "    n = len(y)\n",
    "    for value in set(col):\n",
    "        y_predict = col < value\n",
    "        my_entropy = get_entropy(y_predict, y)\n",
    "        if my_entropy <= min_entropy:\n",
    "            min_entropy = my_entropy\n",
    "            cutoff = value\n",
    "    return min_entropy, cutoff\n",
    "  \n",
    "  def all_same(self, items):\n",
    "    return all(x == items[0] for x in items)\n",
    "                                           \n",
    "  def predict(self, x):\n",
    "    tree = self.trees\n",
    "    results = np.array([0]*len(x))\n",
    "    for i, c in enumerate(x):\n",
    "        results[i] = self.prediction(c)\n",
    "    return results\n",
    "  \n",
    "  def prediction(self, row):\n",
    "    cur_layer = self.trees\n",
    "    while cur_layer.get('cutoff'):\n",
    "        if row[cur_layer['index_col']] < cur_layer['cutoff']:\n",
    "            cur_layer = cur_layer['left']\n",
    "        else:\n",
    "            cur_layer = cur_layer['right']\n",
    "    else:\n",
    "        return cur_layer.get('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iNR-8ivsawWI",
    "outputId": "a7496a3d-ac8a-4379-c97d-c6660d73ec14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information gain :  0.35\n",
      "Information gain :  0.5\n",
      "Information gain :  1.0\n",
      "{'col': 'Salary ($)',\n",
      " 'cutoff': 38000,\n",
      " 'index_col': 1,\n",
      " 'left': {'val': 0},\n",
      " 'right': {'col': 'Age',\n",
      "           'cutoff': 43,\n",
      "           'index_col': 0,\n",
      "           'left': {'val': 1},\n",
      "           'right': {'col': 'Salary ($)',\n",
      "                     'cutoff': 65000,\n",
      "                     'index_col': 1,\n",
      "                     'left': {'val': 0},\n",
      "                     'right': {'val': 1},\n",
      "                     'val': 0.0},\n",
      "           'val': 1.0},\n",
      " 'val': 1.0}\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeAlgo(max_depth=7)\n",
    "tree = clf.fit(X, y)\n",
    "pprint(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQNHwUVXd5Ov"
   },
   "source": [
    "Here, we observe that the depth of the tree is 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "C5PDksbmYyDb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g534TMzBiljC"
   },
   "source": [
    "### 2) Multivariate decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5UT69C5if-Z"
   },
   "source": [
    "A multivariate decision tree is a generalization of univariate decision trees, where more than one\n",
    "attribute can be used in the decision rule for each split. That is, splits need not be orthogonal to a\n",
    "feature’s axis.\n",
    "For the same data, learn a multivariate decision tree where each decision rule is a linear classifier that\n",
    "makes decisions based on the sign of αxage + βxincome − 1. Draw your tree, including the α, β and the\n",
    "information gain for each split.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjcasheSPji6"
   },
   "source": [
    "CART algorithm for decisions tree can be made into a Multivariate.To create a Multivariant tree we need to compute the best split at each node, but instead of throwing away all splits that weren't the best we take a portion of those (maybe all), then evaluate all of the data's attributes by each of the potential splits at that node weighted by the order. So the first split (which lead to the maximum gain) is weighted at 1. Then the next highest gain split is weighted by some fraction < 1.0, and so on. Where the weights decrease as the gain of that split decreases. That number is then compared to same calculation of the nodes within the left node if it's above that number go left. Otherwise go right. This can be a multi-variant split for decision trees. Adding multivariate splits expands the search space enormously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XoADRq1ip5_"
   },
   "source": [
    "### 3) Multivariate decision trees vs Univariate decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOlPSQC0ivcL"
   },
   "source": [
    "Advantages\n",
    "\n",
    "1) Univariate means involving one variate or variable quantity. On the other hand, multivariate means using multiple or more than one variate or variable quantity at the same time. Therefore, multivariate decision trees can use use all attributes at one node when branching\n",
    "\n",
    "2) If the data set is small or if one class has a signiﬁcantly larger number of instancescompared to other classes, then a univariate technique does not overﬁt and can be sufficient\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Disadvantages\n",
    "\n",
    "1) Hyperplanes with an arbitrary orientation are used in multivariate trees. It means that there can be 2^d {N choose d} possible Hyperplanes (where d is the number of dimensions and N is the number of possible thresholds for the split points) which makes exhaustive search inefficient and impractical. Consequently, a more practical way to follow is using linear multivariate node that takes weights for each attribute and sums them up . Moreover, linear multivariate decision trees choose the most important attributes amongst all so that the process would become more efficient and practical.\n",
    "\n",
    "2) If the variables are highly correlated, then a univariate method is not sufficient and we may resort to multivariate methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_KnAGUIWi_5k"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AML_HW4_AkhilaS.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
